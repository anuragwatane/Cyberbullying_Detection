{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "68d1c391-99d8-497f-9f80-9db57a647d4a",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "bert-model-on-english-dataset_binary-classes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3ad1343dbd494b65ae57d13196de0098": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_466434a43406410ca64ae1f0e847cc23",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bff1891d7a7b48b8981d2ef90803e568",
              "IPY_MODEL_2b3e6c71301a4029a60f481e64f94727"
            ]
          }
        },
        "466434a43406410ca64ae1f0e847cc23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bff1891d7a7b48b8981d2ef90803e568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8e531f7ed7b24be19c7dd02950b5cf61",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 570,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 570,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_093eff95caa14cdc9556eed9429db1d1"
          }
        },
        "2b3e6c71301a4029a60f481e64f94727": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cdf56c2be44a4d1f9843377e080bce92",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 570/570 [00:09&lt;00:00, 61.4B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d7b1d384e12d4066857212b0f37d4f7c"
          }
        },
        "8e531f7ed7b24be19c7dd02950b5cf61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "093eff95caa14cdc9556eed9429db1d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cdf56c2be44a4d1f9843377e080bce92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d7b1d384e12d4066857212b0f37d4f7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bd2e11dcd7a94c6f94c1a121f699748f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_34e3c2439ce14e5b8721d02529497516",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5289ad276a8d42f393ea8f8507c49bdf",
              "IPY_MODEL_cab73c1c437d4f2f9801baf70f8ec34a"
            ]
          }
        },
        "34e3c2439ce14e5b8721d02529497516": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5289ad276a8d42f393ea8f8507c49bdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f0482c82b02348509ba5f61512e483f5",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f29f2fc6598447d3aa75214702351f55"
          }
        },
        "cab73c1c437d4f2f9801baf70f8ec34a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6015f676cbc6437bb2cb002e027fd718",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:08&lt;00:00, 50.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_577c96dbbb1246cd82074f1cf534ba24"
          }
        },
        "f0482c82b02348509ba5f61512e483f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f29f2fc6598447d3aa75214702351f55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6015f676cbc6437bb2cb002e027fd718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "577c96dbbb1246cd82074f1cf534ba24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_x2MiA38sb0",
        "outputId": "b75eee9a-1181-4eeb-abac-5563a1706f18"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "13607867-4742-474a-a132-addb5178094d",
        "tags": [],
        "id": "UksEy5Fq7YdW"
      },
      "source": [
        "!pip install nltk\n",
        "!pip install bert-tensorflow\n",
        "!pip install transformers\n",
        "!pip install seaborn\n",
        "!pip install -U sentence-transformers\n",
        "!pip install pytorch-pretrained-bert\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "9a47cf69-bed6-4ad2-a22e-4386431ddeba",
        "tags": [],
        "id": "WRYyV5JM7YdY"
      },
      "source": [
        "# Insert code here.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "import datetime\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, neighbors\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sentence_transformers import SentenceTransformer\n",
        "sent_encoder = SentenceTransformer('bert-base-nli-mean-tokens')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "ed86072d-0baf-4960-90f2-1aaa7943c54a",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UcUyQ1B7YdZ",
        "outputId": "eca11454-58c5-4e8c-9f62-f502eee906b7"
      },
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")  # identify and specify the GPU as the device\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No GPU available, using the CPU instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "9cd08a9a-f2f9-4e71-80eb-056a1b986cbe",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiS7YtGZ7Yda",
        "outputId": "1f171e8d-1b6e-4d45-c00e-9723762b73e6"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "b0583ff8-dd94-4ace-996b-1e86e9038285",
        "tags": [],
        "id": "73QpxF7f7Yda"
      },
      "source": [
        "DATASET_PATH = '/content/drive/MyDrive/Colab Notebooks/Hate-Speech-Detection/data/2020_processed_train/en.pickle'\n",
        "MODEL_NAME = '/content/drive/MyDrive/Colab Notebooks/Hate-Speech-Detection/models/task1_model_by_epoch/bert-base-en-task1-epoch-'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGI0XTxP7Ydb"
      },
      "source": [
        "# Load the pickle file\n",
        "with open(DATASET_PATH,'rb') as f:\n",
        "    data = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHIp5FyG7Ydb"
      },
      "source": [
        "# create a dataframe using pickle file\n",
        "df = pd.DataFrame.from_dict(data)\n",
        "# df = pd.read_csv(DATASET_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "ec9990b7-1c2f-4cb4-b062-a9ae2dd58d7f",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1204
        },
        "id": "4fLQ5n2n7Ydc",
        "outputId": "f7ddab5e-60ce-4e99-93f8-61fa30d8518e"
      },
      "source": [
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "%load_ext google.colab.data_table\n",
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 3,708\n",
            "\n",
            "The google.colab.data_table extension is already loaded. To reload it, use:\n",
            "  %reload_ext google.colab.data_table\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 3439,\n            'f': \"3439\",\n        },\n\"1126915725355098112\",\n\"HOF\",\n\"OFFN\",\n\"hasoc_2020_en_4923\",\n\"RT @NoyeluE: When a girl suddenly stop singing while she's bathing...\\nMy brother, just know that she's washing her Iskaba Iskelebete Iskolo\\u2026\",\n\": When a girl suddenly stop singing while she's bathing...My brother, just know that she's washing her Iskaba Iskelebete Iskolo\",\n[],\n[],\n[],\n[],\n[\"@NoyeluE\"],\n[],\n[\"RT\"],\n[],\n[]],\n [{\n            'v': 1067,\n            'f': \"1067\",\n        },\n\"1130255595079180290\",\n\"NOT\",\n\"NONE\",\n\"hasoc_2020_en_233\",\n\"RT @GFFN: Kylian Mbapp\\u00e9 at 20:\\n- 1 x World Cup\\n- 1 x U19 Euros\\n- 3 x Ligue 1\\n- 1 x Coupe de la Ligue\\n- 1 x Coupe de France\\n- 1 x Troph\\u00e9e de\\u2026\",\n\": Kylian Mbapp at :- x World Cup- x U19 Euros- x Ligue - x Coupe de la Ligue- x Coupe de France- x Trophe de\",\n[],\n[],\n[],\n[],\n[\"@GFFN\"],\n[\" 20\", \" 1\", \" 1\", \" 3\", \" 1\", \" 1\", \" 1\", \" 1\"],\n[\"RT\"],\n[],\n[]],\n [{\n            'v': 2468,\n            'f': \"2468\",\n        },\n\"1123657162172059649\",\n\"NOT\",\n\"NONE\",\n\"hasoc_2020_en_88\",\n\"@WB_Baskerville Excellent point. \\n\\nAbove: title page of dissertation of the late Father Victor De Clercq, C.I.C.M.,\\u2026 https://t.co/3DHdQHQKka\",\n\"Excellent point. Above: title page of dissertation of the late Father Victor De Clercq, C.I.C.M.,\",\n[],\n[],\n[],\n[\"https://t.co/3DHdQHQKka\"],\n[\"@WB_Baskerville\"],\n[],\n[],\n[],\n[]],\n [{\n            'v': 1070,\n            'f': \"1070\",\n        },\n\"1123799235852414979\",\n\"HOF\",\n\"PRFN\",\n\"hasoc_2020_en_2987\",\n\"Ah shit mtvs running a little late\",\n\"Ah shit mtvs running a little late\",\n[],\n[],\n[],\n[],\n[],\n[],\n[],\n[],\n[]],\n [{\n            'v': 1156,\n            'f': \"1156\",\n        },\n\"1130032806258593792\",\n\"NOT\",\n\"NONE\",\n\"hasoc_2020_en_4835\",\n\"RT @alamgirizvi: I think it's our responsibility to tell everyone about our PM where doing Meditation \\u26e9\\ud83d\\udcff\\ud83d\\udd49\\ud83d\\udc4c\\n\\nExcellent details about the Mod\\u2026\",\n\": I think it's our responsibility to tell everyone about our PM where doing Meditation Excellent details about the Mod\",\n[],\n[],\n[\"\\u26e9\", \"\\ud83d\\udcff\", \"\\ud83d\\udd49\", \"\\ud83d\\udc4c\"],\n[],\n[\"@alamgirizvi\"],\n[],\n[\"RT\"],\n[\"shinto shrine\", \"prayer beads\", \"om\", \"OK hand\"],\n[]],\n [{\n            'v': 326,\n            'f': \"326\",\n        },\n\"1126912978077540353\",\n\"NOT\",\n\"NONE\",\n\"hasoc_2020_en_4603\",\n\"RT @AphraBrandreth: You don\\u2019t need an excuse to #CelebrateChessington but cakes and face painting are always a bonus https://t.co/uV0NfXFDLz\",\n\": You dont need an excuse to but cakes and face painting are always a bonus\",\n[\"#CelebrateChessington\"],\n[],\n[],\n[\"https://t.co/uV0NfXFDLz\"],\n[\"@AphraBrandreth\"],\n[],\n[\"RT\"],\n[],\n[\"celebrate chessington\"]],\n [{\n            'v': 3199,\n            'f': \"3199\",\n        },\n\"1123498416183631872\",\n\"NOT\",\n\"NONE\",\n\"hasoc_2020_en_5105\",\n\"I don\\u2019t want to say it, but the fact that we could be playing Tranmere in the play offs no matter what happens scar\\u2026 https://t.co/hbffui8Fm1\",\n\"I dont want to say it, but the fact that we could be playing Tranmere in the play offs no matter what happens scar\",\n[],\n[],\n[],\n[\"https://t.co/hbffui8Fm1\"],\n[],\n[],\n[],\n[],\n[]],\n [{\n            'v': 1933,\n            'f': \"1933\",\n        },\n\"1126928568305348608\",\n\"NOT\",\n\"NONE\",\n\"hasoc_2020_en_1884\",\n\"RT @stfuIol: over 7 billion people in this world and you think I'm gonna chase someone who doesn't even want me? hahah\\n\\nthat's exactly what\\u2026\",\n\": over billion people in this world and you think I'm gonna chase someone who doesn't even want me? hahahthat's exactly what\",\n[],\n[],\n[],\n[],\n[\"@stfuIol\"],\n[\" 7\"],\n[\"RT\"],\n[],\n[]],\n [{\n            'v': 836,\n            'f': \"836\",\n        },\n\"1127022029985595392\",\n\"NOT\",\n\"NONE\",\n\"hasoc_2020_en_2158\",\n\"you put the \\\"utu\\\" in OUR \\\"future\\\"\\ud83d\\ude09\\ud83d\\ude02\",\n\"you put the \\\"utu\\\" in OUR \\\"future\\\"\",\n[],\n[],\n[\"\\ud83d\\ude09\", \"\\ud83d\\ude02\"],\n[],\n[],\n[],\n[],\n[\"winking face\", \"face with tears of joy\"],\n[]],\n [{\n            'v': 203,\n            'f': \"203\",\n        },\n\"1130129581434908674\",\n\"HOF\",\n\"OFFN\",\n\"hasoc_2020_en_3890\",\n\"RT @CrypticNoOne: Who's the dumbass that drop kicked Arnold Schwarzenegger?? We need names\\n\\nhttps://t.co/TrEQeTiHi6\",\n\": Who's the dumbass that drop kicked Arnold Schwarzenegger?? We need names\",\n[],\n[],\n[],\n[\"https://t.co/TrEQeTiHi6\"],\n[\"@CrypticNoOne\"],\n[],\n[\"RT\"],\n[],\n[]]],\n        columns: [[\"number\", \"index\"], [\"string\", \"tweet_id\"], [\"string\", \"task_1\"], [\"string\", \"task_2\"], [\"string\", \"hasoc_id\"], [\"string\", \"full_tweet\"], [\"string\", \"tweet_raw_text\"], [\"string\", \"hashtags\"], [\"string\", \"smiley\"], [\"string\", \"emoji\"], [\"string\", \"url\"], [\"string\", \"mentions\"], [\"string\", \"numerals\"], [\"string\", \"reserved_word\"], [\"string\", \"emotext\"], [\"string\", \"segmented_hash\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>task_1</th>\n",
              "      <th>task_2</th>\n",
              "      <th>hasoc_id</th>\n",
              "      <th>full_tweet</th>\n",
              "      <th>tweet_raw_text</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>smiley</th>\n",
              "      <th>emoji</th>\n",
              "      <th>url</th>\n",
              "      <th>mentions</th>\n",
              "      <th>numerals</th>\n",
              "      <th>reserved_word</th>\n",
              "      <th>emotext</th>\n",
              "      <th>segmented_hash</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3439</th>\n",
              "      <td>1126915725355098112</td>\n",
              "      <td>HOF</td>\n",
              "      <td>OFFN</td>\n",
              "      <td>hasoc_2020_en_4923</td>\n",
              "      <td>RT @NoyeluE: When a girl suddenly stop singing...</td>\n",
              "      <td>: When a girl suddenly stop singing while she'...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[@NoyeluE]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[RT]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1067</th>\n",
              "      <td>1130255595079180290</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>hasoc_2020_en_233</td>\n",
              "      <td>RT @GFFN: Kylian MbappÃ© at 20:\\n- 1 x World Cu...</td>\n",
              "      <td>: Kylian Mbapp at :- x World Cup- x U19 Euros-...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[@GFFN]</td>\n",
              "      <td>[ 20,  1,  1,  3,  1,  1,  1,  1]</td>\n",
              "      <td>[RT]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2468</th>\n",
              "      <td>1123657162172059649</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>hasoc_2020_en_88</td>\n",
              "      <td>@WB_Baskerville Excellent point. \\n\\nAbove: ti...</td>\n",
              "      <td>Excellent point. Above: title page of disserta...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[https://t.co/3DHdQHQKka]</td>\n",
              "      <td>[@WB_Baskerville]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1070</th>\n",
              "      <td>1123799235852414979</td>\n",
              "      <td>HOF</td>\n",
              "      <td>PRFN</td>\n",
              "      <td>hasoc_2020_en_2987</td>\n",
              "      <td>Ah shit mtvs running a little late</td>\n",
              "      <td>Ah shit mtvs running a little late</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1156</th>\n",
              "      <td>1130032806258593792</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>hasoc_2020_en_4835</td>\n",
              "      <td>RT @alamgirizvi: I think it's our responsibili...</td>\n",
              "      <td>: I think it's our responsibility to tell ever...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[â›©, ðŸ“¿, ðŸ•‰, ðŸ‘Œ]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[@alamgirizvi]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[RT]</td>\n",
              "      <td>[shinto shrine, prayer beads, om, OK hand]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>326</th>\n",
              "      <td>1126912978077540353</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>hasoc_2020_en_4603</td>\n",
              "      <td>RT @AphraBrandreth: You donâ€™t need an excuse t...</td>\n",
              "      <td>: You dont need an excuse to but cakes and fac...</td>\n",
              "      <td>[#CelebrateChessington]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[https://t.co/uV0NfXFDLz]</td>\n",
              "      <td>[@AphraBrandreth]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[RT]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[celebrate chessington]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3199</th>\n",
              "      <td>1123498416183631872</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>hasoc_2020_en_5105</td>\n",
              "      <td>I donâ€™t want to say it, but the fact that we c...</td>\n",
              "      <td>I dont want to say it, but the fact that we co...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[https://t.co/hbffui8Fm1]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1933</th>\n",
              "      <td>1126928568305348608</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>hasoc_2020_en_1884</td>\n",
              "      <td>RT @stfuIol: over 7 billion people in this wor...</td>\n",
              "      <td>: over billion people in this world and you th...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[@stfuIol]</td>\n",
              "      <td>[ 7]</td>\n",
              "      <td>[RT]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836</th>\n",
              "      <td>1127022029985595392</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "      <td>hasoc_2020_en_2158</td>\n",
              "      <td>you put the \"utu\" in OUR \"future\"ðŸ˜‰ðŸ˜‚</td>\n",
              "      <td>you put the \"utu\" in OUR \"future\"</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[ðŸ˜‰, ðŸ˜‚]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[winking face, face with tears of joy]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>1130129581434908674</td>\n",
              "      <td>HOF</td>\n",
              "      <td>OFFN</td>\n",
              "      <td>hasoc_2020_en_3890</td>\n",
              "      <td>RT @CrypticNoOne: Who's the dumbass that drop ...</td>\n",
              "      <td>: Who's the dumbass that drop kicked Arnold Sc...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[https://t.co/TrEQeTiHi6]</td>\n",
              "      <td>[@CrypticNoOne]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[RT]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 tweet_id  ...           segmented_hash\n",
              "3439  1126915725355098112  ...                       []\n",
              "1067  1130255595079180290  ...                       []\n",
              "2468  1123657162172059649  ...                       []\n",
              "1070  1123799235852414979  ...                       []\n",
              "1156  1130032806258593792  ...                       []\n",
              "326   1126912978077540353  ...  [celebrate chessington]\n",
              "3199  1123498416183631872  ...                       []\n",
              "1933  1126928568305348608  ...                       []\n",
              "836   1127022029985595392  ...                       []\n",
              "203   1130129581434908674  ...                       []\n",
              "\n",
              "[10 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "5be4d068-10df-4c23-8b5d-cb43dbc3d438",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmZwxHg-7Ydd",
        "outputId": "c54190a6-693d-46da-f4b0-d98d038e9157"
      },
      "source": [
        "# convert labels (categorical values) into numbers\n",
        "LE = LabelEncoder()\n",
        "\n",
        "df['task_1'] = LE.fit_transform(df['task_1'])\n",
        "task1_params = dict(zip(LE.classes_, LE.transform(LE.classes_)))\n",
        "print(task1_params)\n",
        "\n",
        "df['task_2'] = LE.fit_transform(df['task_2'])\n",
        "task2_params = dict(zip(LE.classes_, LE.transform(LE.classes_)))\n",
        "print(task2_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'HOF': 0, 'NOT': 1}\n",
            "{'HATE': 0, 'NONE': 1, 'OFFN': 2, 'PRFN': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "c9cfc3cf-9bab-44a9-b5a9-b6df80db4b89",
        "tags": [],
        "id": "26ohVdEL7Yde"
      },
      "source": [
        "def count_words(text):\n",
        "    return len(text.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "43ee2895-f2cb-4fae-baf8-8f42ae25c334",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRCnnNlc7Yde",
        "outputId": "cc701c1e-ebaa-475a-923d-54de518702f1"
      },
      "source": [
        "df.tweet_raw_text.apply(count_words).max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "3360f571-ba0c-425d-8359-a803a11cee5e",
        "tags": [],
        "id": "Mv1kgANB7Yde"
      },
      "source": [
        "MAX_LENGTH = 40\n",
        "posts = df.tweet_raw_text.values\n",
        "\n",
        "# store binary categories for each text\n",
        "categories = df['task_1'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "3d0175d1-ae28-4f6d-8a57-6562ddd4fb00",
        "output_cleared": false,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "barKTCSu7Ydf",
        "outputId": "57bfe567-c06b-4f23-e44d-0f3b616885f4"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in posts:\n",
        "    # 'encode_plus' will:\n",
        "    #   (1) Tokenize the sentence. Split the sentence into tokens.\n",
        "    #   (2) Prepend the [CLS] token to the start.\n",
        "    #   (3) Append the [SEP] token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to 'max_length'\n",
        "    #   (6) Create the attention masks which explicitly differentiate real tokens from [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                           # Sentence to encode.\n",
        "                        add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = MAX_LENGTH,        # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',          # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(categories)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "c2cecba9-0f4e-478b-944b-2e06cd739d23",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7vRi_c47Ydg",
        "outputId": "7dbf7230-8566-4438-a184-c5e07c174dc1"
      },
      "source": [
        "print('Original: ', posts[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  hate wen females hit ah nigga with tht bro , Im tryna make u my la sweety , fuck ah bro\n",
            "Token IDs: tensor([  101,  5223, 19181,  3801,  2718,  6289,  9152, 23033,  2007, 16215,\n",
            "         2102, 22953,  1010, 10047,  3046,  2532,  2191,  1057,  2026,  2474,\n",
            "         4086,  2100,  1010,  6616,  6289, 22953,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "32d4c338-62d2-4381-895f-d843394efbea",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxXj4udR7Ydg",
        "outputId": "8effe311-b0ff-438d-b62d-3d00ae23faf2"
      },
      "source": [
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 80-20 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.80 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2,966 training samples\n",
            "  742 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "fad6e35a-d6d9-4e48-9155-5f4c308f0790",
        "tags": [],
        "id": "Zmd1se0w7Ydh"
      },
      "source": [
        "# The DataLoader needs to know our batch size for training, so we specify it here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "d490c2c0-4a4a-439a-8eb7-797e9cbea099",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218,
          "referenced_widgets": [
            "3ad1343dbd494b65ae57d13196de0098",
            "466434a43406410ca64ae1f0e847cc23",
            "bff1891d7a7b48b8981d2ef90803e568",
            "2b3e6c71301a4029a60f481e64f94727",
            "8e531f7ed7b24be19c7dd02950b5cf61",
            "093eff95caa14cdc9556eed9429db1d1",
            "cdf56c2be44a4d1f9843377e080bce92",
            "d7b1d384e12d4066857212b0f37d4f7c",
            "bd2e11dcd7a94c6f94c1a121f699748f",
            "34e3c2439ce14e5b8721d02529497516",
            "5289ad276a8d42f393ea8f8507c49bdf",
            "cab73c1c437d4f2f9801baf70f8ec34a",
            "f0482c82b02348509ba5f61512e483f5",
            "f29f2fc6598447d3aa75214702351f55",
            "6015f676cbc6437bb2cb002e027fd718",
            "577c96dbbb1246cd82074f1cf534ba24"
          ]
        },
        "id": "sSn6xHoV7Ydi",
        "outputId": "565a0254-4cc6-44dd-afe8-f5798348fb50"
      },
      "source": [
        "# Load BertForSequenceClassification,\n",
        "# the pretrained BERT model with a single linear classification layer on top. \n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "  # Tell pytorch to run this model on the GPU.\n",
        "  model.cuda()\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ad1343dbd494b65ae57d13196de0098",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd2e11dcd7a94c6f94c1a121f699748f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdsC-2887Ydk"
      },
      "source": [
        "# import torch.nn as nn\n",
        "# if torch.cuda.device_count() > 1:\n",
        "#     print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "#     # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
        "#     model = nn.DataParallel(model)\n",
        "\n",
        "# model.to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-uK9PL90ElO"
      },
      "source": [
        "We can browse all of the modelâ€™s parameters by name here.\n",
        "\n",
        "In the below cell, Iâ€™ve printed out the names and dimensions of the weights for:\n",
        "1. The embedding layer.\n",
        "2. The first of the twelve transformers.\n",
        "3. The output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "bbf50601-f951-4f28-9d05-8e4627e6c2ad",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGwdUzpa7Ydl",
        "outputId": "0d144e28-279c-4238-9177-0c7ac2679864"
      },
      "source": [
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "d874d9d0-5d56-4e01-b070-be8d20e4bdfe",
        "tags": [],
        "id": "Hsl9G-6o7Ydm"
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "            )\n",
        "\n",
        "# The epsilon parameter eps = 1e-8 is â€œa very small number to prevent any division by zero in the implementationâ€"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "b040ff6a-a330-4fe8-8860-073c2eb16854",
        "tags": [],
        "id": "tSCn6sFR7Ydm"
      },
      "source": [
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the training data.\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "638216a4-a64a-4c51-8b26-96792793414c",
        "tags": [],
        "id": "sTagdfgS7Ydm"
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "07698c0d-6066-48f1-8b5f-88f0a221dcf6",
        "tags": [],
        "id": "WckTzciT7Ydn"
      },
      "source": [
        "# Helper function for formatting elapsed times as hh:mm:ss\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "b19a446f-890a-4729-9bb4-25d96fe886b9",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E98nsq3e7Ydn",
        "outputId": "40bc3d48-59ec-4c68-9ddf-bb5b1c2c3de3"
      },
      "source": [
        "seed_val = 42\n",
        "torch.cuda.empty_cache()\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        \"\"\"  # start - commented by anurag\n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "        \"\"\"  # end - commented by anurag\n",
        "\n",
        "        # start - added by anurag\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = outputs.loss\n",
        "        # print(f\"loss: {loss.item()}\")\n",
        "        # print(f\"logits: {output.logits}\")\n",
        "        # end - added by anurag\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    torch.save(model, MODEL_NAME+str(epoch_i))\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            \"\"\"  # start - commented by anurag\n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \"\"\"  # end - commented by anurag\n",
        "\n",
        "            # start - added by anurag\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "            # print(f\"loss: {loss}\")\n",
        "            # print(f\"logits: {logits}\")\n",
        "            # end - added by anurag\n",
        "\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     93.    Elapsed: 0:00:09.\n",
            "  Batch    80  of     93.    Elapsed: 0:00:18.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epcoh took: 0:00:21\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.28\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     93.    Elapsed: 0:00:09.\n",
            "  Batch    80  of     93.    Elapsed: 0:00:19.\n",
            "\n",
            "  Average training loss: 0.22\n",
            "  Training epcoh took: 0:00:22\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.89\n",
            "  Validation Loss: 0.29\n",
            "  Validation took: 0:00:21\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     93.    Elapsed: 0:00:10.\n",
            "  Batch    80  of     93.    Elapsed: 0:00:20.\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epcoh took: 0:00:23\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.89\n",
            "  Validation Loss: 0.32\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     93.    Elapsed: 0:00:10.\n",
            "  Batch    80  of     93.    Elapsed: 0:00:21.\n",
            "\n",
            "  Average training loss: 0.11\n",
            "  Training epcoh took: 0:00:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.33\n",
            "  Validation took: 0:00:09\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:02:19 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS9uHUas793z"
      },
      "source": [
        "Summary of the training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "9326a12f-0a1a-445c-89ef-10f992693b05",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "FQ6eqgR-7Ydr",
        "outputId": "0e03e14e-a0ac-450a-ab53-b2216a49d19a"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.39211088939700073,\n            'f': \"0.39211088939700073\",\n        },\n{\n            'v': 0.2827492964764436,\n            'f': \"0.2827492964764436\",\n        },\n{\n            'v': 0.9010416666666666,\n            'f': \"0.9010416666666666\",\n        },\n\"0:00:21\",\n\"0:00:10\"],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 0.22071518897209116,\n            'f': \"0.22071518897209116\",\n        },\n{\n            'v': 0.29032372248669464,\n            'f': \"0.29032372248669464\",\n        },\n{\n            'v': 0.8919270833333334,\n            'f': \"0.8919270833333334\",\n        },\n\"0:00:22\",\n\"0:00:21\"],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 0.14879762892040513,\n            'f': \"0.14879762892040513\",\n        },\n{\n            'v': 0.32466341570640606,\n            'f': \"0.32466341570640606\",\n        },\n{\n            'v': 0.8880208333333334,\n            'f': \"0.8880208333333334\",\n        },\n\"0:00:23\",\n\"0:00:09\"],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 0.10816975138200227,\n            'f': \"0.10816975138200227\",\n        },\n{\n            'v': 0.3289907455133895,\n            'f': \"0.3289907455133895\",\n        },\n{\n            'v': 0.8984375,\n            'f': \"0.8984375\",\n        },\n\"0:00:24\",\n\"0:00:09\"]],\n        columns: [[\"number\", \"epoch\"], [\"number\", \"Training Loss\"], [\"number\", \"Valid. Loss\"], [\"number\", \"Valid. Accur.\"], [\"string\", \"Training Time\"], [\"string\", \"Validation Time\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.39</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0:00:21</td>\n",
              "      <td>0:00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.22</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0:00:22</td>\n",
              "      <td>0:00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.15</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0:00:23</td>\n",
              "      <td>0:00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.11</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0:00:24</td>\n",
              "      <td>0:00:09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               0.39         0.28           0.90       0:00:21         0:00:10\n",
              "2               0.22         0.29           0.89       0:00:22         0:00:21\n",
              "3               0.15         0.32           0.89       0:00:23         0:00:09\n",
              "4               0.11         0.33           0.90       0:00:24         0:00:09"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "6b3bafe5-50f3-4e48-b583-cace811841b9",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "97V4wmYk7Ydv",
        "outputId": "07cb2778-610f-42b8-9ba0-298ef40de67e"
      },
      "source": [
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8JQbPqQSqOh"
      },
      "source": [
        "The training loss is going down with each epoch, the validation loss is increasing! This suggests that we are training our model too long, and itâ€™s over-fitting on the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPZTVyyHUUbQ"
      },
      "source": [
        "**Performance On Test Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4SCJb2l7Ydw"
      },
      "source": [
        "TEST_PATH = '/content/drive/MyDrive/Colab Notebooks/Hate-Speech-Detection/data/2020_processed_test/en_test.pickle'\n",
        "\n",
        "with open(TEST_PATH,'rb') as f:\n",
        "    data_test = pickle.load(f)\n",
        "\n",
        "df_test = pd.DataFrame.from_dict(data_test)\n",
        "\n",
        "# Create sentence and label lists\n",
        "posts = df_test.tweet_raw_text.values\n",
        "\n",
        "categories = []\n",
        "\n",
        "for i in df_test.task_1:\n",
        "    categories.append(task1_params[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGiGbEbBW3b6"
      },
      "source": [
        "Data Preparation\n",
        "\n",
        "Weâ€™ll need to apply all of the same steps that we did for the training data to prepare our test data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "becae5e1-6875-4313-92b1-3ca47003249d",
        "output_cleared": false,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORW80Tlw7Ydx",
        "outputId": "e80dc18f-66cc-42ec-d93e-e4e691309ad4"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in posts:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                           # Sentence to encode.\n",
        "                        add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = MAX_LENGTH,        # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',          # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(categories)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "7bb0275c-9fbe-457c-9183-65d8b8181ff5",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1l3EM-CP7Ydy",
        "outputId": "affc2c96-f1d5-44b1-defa-c2d1382252dd"
      },
      "source": [
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('\\nDONE')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 814 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "226bdc8f-7717-4107-80d4-e0bc292e7176",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ah9XL3-87Ydy",
        "outputId": "14629cad-f899-42f3-855e-e3d354d7b75a"
      },
      "source": [
        "print(predictions[0],true_labels[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-2.4018703   2.2060192 ]\n",
            " [-2.3457365   2.3349802 ]\n",
            " [-2.297114    2.061532  ]\n",
            " [ 2.5259047  -2.2720265 ]\n",
            " [ 2.4577847  -2.1060038 ]\n",
            " [-1.6005025   1.4974406 ]\n",
            " [ 2.5584817  -2.2886329 ]\n",
            " [ 2.646024   -2.2902195 ]\n",
            " [-0.6533378   0.8363467 ]\n",
            " [ 2.3279676  -2.1756597 ]\n",
            " [ 2.400221   -2.1513567 ]\n",
            " [ 2.5979981  -2.2410073 ]\n",
            " [ 2.311139   -2.0655768 ]\n",
            " [-2.3410292   2.1960187 ]\n",
            " [ 0.7221781  -0.21297485]\n",
            " [ 2.3980522  -1.966161  ]] [1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "001ecf3b-4a00-424d-bf40-bfa6b235126c",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqDqEJsj7Yd0",
        "outputId": "63027742-e21f-4394-8255-5c8c4e46ff78"
      },
      "source": [
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('Total MCC: %.3f' % mcc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total MCC: 0.780\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "7a1de298-5012-436b-b25e-418e45d0c64a",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfomhBu47Yd1",
        "outputId": "53348a93-8f1d-42c3-cd7f-8173b3ca5221"
      },
      "source": [
        "accurate = 0\n",
        "for (i,j) in zip(flat_predictions, flat_true_labels):\n",
        "    if i==j:\n",
        "        accurate += 1\n",
        "accurate/len(flat_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8894348894348895"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "288df715-d267-4372-b62f-acb4e217dfe4",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiJBjdwA7Yd2",
        "outputId": "3485ddde-24a2-4352-c187-719a99f0272c"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1_score(flat_true_labels, flat_predictions, average='macro')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8894182002608317"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O32iHKsKPtwI"
      },
      "source": [
        "##Method 1 to save and load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTLZnh-dT-9Q",
        "outputId": "850399ab-7ead-4594-ec2d-d0e979b90ef2"
      },
      "source": [
        "import os\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/Colab Notebooks/Hate-Speech-Detection/models/model_plus_tokenizer_task1/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to /content/drive/MyDrive/Colab Notebooks/Hate-Speech-Detection/models/model_plus_tokenizer_task1/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Colab Notebooks/Hate-Speech-Detection/models/model_plus_tokenizer_task1/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/Hate-Speech-Detection/models/model_plus_tokenizer_task1/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/Hate-Speech-Detection/models/model_plus_tokenizer_task1/vocab.txt',\n",
              " '/content/drive/MyDrive/Colab Notebooks/Hate-Speech-Detection/models/model_plus_tokenizer_task1/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_1XzWMZJx05"
      },
      "source": [
        "def classify_sentence(sentence):\n",
        "\n",
        "    input_id_sentence = []\n",
        "    attention_mask_sentence = []\n",
        "\n",
        "    model = BertForSequenceClassification.from_pretrained(output_dir)\n",
        "\n",
        "    print(f\"\\nThe model can predict {model.num_labels} different classes\\n\")\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                sentence,                      # Sentence to encode.\n",
        "                add_special_tokens = True,     # Add '[CLS]' and '[SEP]'\n",
        "                max_length = MAX_LENGTH,       # Pad & truncate all sentences.\n",
        "                pad_to_max_length = True,\n",
        "                return_attention_mask = True,  # Construct attn. masks.\n",
        "                return_tensors = 'pt',         # Return pytorch tensors.\n",
        "    )\n",
        "\n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_id_sentence.append(encoded_dict['input_ids'])\n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_mask_sentence.append(encoded_dict['attention_mask'])\n",
        "    \n",
        "    input_id_sentence = torch.cat(input_id_sentence, dim=0)\n",
        "    attention_mask_sentence = torch.cat(attention_mask_sentence, dim=0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_id_sentence, token_type_ids=None, attention_mask=attention_mask_sentence)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    print(\"\\n The normalized probability for each class is\")\n",
        "    print(logits.softmax(dim=-1).tolist())\n",
        "    pred_class_prob = logits.softmax(dim=-1).tolist()\n",
        "\n",
        "    pred_label_from_prob = np.argmax(pred_class_prob)\n",
        "    print(f\"\\nThe predicted class is: {pred_label_from_prob}\")\n",
        "\n",
        "    return pred_label_from_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc11DC9eJ8pQ",
        "outputId": "e5965917-9560-43b8-804a-7493337d0260"
      },
      "source": [
        "# Example #1\n",
        "text = \"\"\"\n",
        "You are bad\n",
        "\"\"\"\n",
        "print(classify_sentence(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The model can predict 2 different classes\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " The normalized probability for each class is\n",
            "[[0.3133687376976013, 0.6866312026977539]]\n",
            "\n",
            "The predicted class is: 1\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMAEffzUnKKk"
      },
      "source": [
        "##Method 2 to save and load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00035-b2885ff3-093f-4a8d-be7a-8ca79dbcedc0",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIizYYPp7Yd3",
        "outputId": "1b77398f-92eb-4c46-a6c8-58a908144798"
      },
      "source": [
        "MODEL_PATH = '/content/drive/MyDrive/Colab Notebooks/Hate-Speech-Detection/models/task1_model_by_epoch/model_english_epoch_'+str(epochs)\n",
        "print(MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Hate-Speech-Detection/models/task1_model_by_epoch/model_english_epoch_4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00034-1956addd-ef03-4b88-ab34-d7be31f4907f",
        "tags": [],
        "id": "o--jjRQK7Yd4"
      },
      "source": [
        "torch.save(model, MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00036-ca8908c4-4ac9-4083-be05-5c99b5c1b8ac",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBCD1hca7Yd4",
        "outputId": "f703b4d5-8459-4055-c8f2-01229473dc96"
      },
      "source": [
        "model = torch.load(MODEL_PATH+'4')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Hate-Speech-Detection/models/bert-base-en-task1-epoch-\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jZwOUoAS_IS"
      },
      "source": [
        "##Method 3 to save and load model\n",
        "\n",
        "https://programmer.help/blogs/transformers-saves-and-loads-the-model.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C2hD8A8TGUw"
      },
      "source": [
        "import os\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/Colab Notebooks/Hate-Speech-Detection/models/task1_model_with_state_dict'\n",
        "\n",
        "model_file_path = os.path.join(output_dir, \"model_file.bin\")\n",
        "config_file_path = os.path.join(output_dir, \"config_file.bin\")\n",
        "vocab_file_path = os.path.join(output_dir, \"vocab_file.bin\")\n",
        "\n",
        "# Step 1: save a fine tuned model, configuration, and glossary\n",
        "\n",
        "#If we have a distributed model, save only the encapsulated model\n",
        "#It is wrapped in PyTorch DistributedDataParallel or DataParallel\n",
        "\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "\n",
        "torch.save(model_to_save.state_dict(), model_file_path)\n",
        "\n",
        "model_to_save.config.to_json_file(config_file_path)\n",
        "\n",
        "tokenizer.save_vocabulary(vocab_file_path)\n",
        "\n",
        "# Step 2: reload the saved model\n",
        "\n",
        "config = BertConfig.from_json_file(config_file_path)\n",
        "\n",
        "model = BertForSequenceClassification(config)\n",
        "\n",
        "state_dict = torch.load(model_file_path)\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "tokenizer = BertTokenizer(vocab_file_path, do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkP0AcNZMifL"
      },
      "source": [
        "##Method 4 to save and load model\n",
        "\n",
        "https://huggingface.co/transformers/v1.0.0/model_doc/overview.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MynSbOFPMqII"
      },
      "source": [
        "import os\n",
        "from pytorch_pretrained_bert import WEIGHTS_NAME, CONFIG_NAME\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/Colab Notebooks/Hate-Speech-Detection/models/task1_model_saved_using_weights_name_config_name\"\n",
        "\n",
        "# Step 1: Save a model, configuration and vocabulary that you have fine-tuned\n",
        "\n",
        "# If we have a distributed model, save only the encapsulated model\n",
        "# (it was wrapped in PyTorch DistributedDataParallel or DataParallel)\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "\n",
        "# If we save using the predefined names, we can load using `from_pretrained`\n",
        "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
        "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
        "\n",
        "torch.save(model_to_save.state_dict(), output_model_file)\n",
        "model_to_save.config.to_json_file(output_config_file)\n",
        "tokenizer.save_vocabulary(output_dir)\n",
        "\n",
        "# Step 2: Re-load the saved model and vocabulary\n",
        "\n",
        "# Example for a Bert model\n",
        "model = BertForSequenceClassification.from_pretrained(output_dir)\n",
        "tokenizer = BertTokenizer.from_pretrained(output_dir)  # Add specific options if needed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfRSfZnmWgBu"
      },
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",          # Use the 12-layer BERT model, with an uncased vocab\n",
        "    num_labels = 2,               # The number of output labels--2 for binary classification\n",
        "    output_attentions = False,    # Whether the model returns attentions weights\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}